# news-clustering

## Description
The main objective of this project is to explore the effectiveness of S-BERT in similarity-based text encoding, with the aim of clustering articles by content similarity. This will later enable advanced analyses. The project also seeks to evaluate whether using S-BERT adds value compared to other text encoding techniques.




## Workflow
1. **Generation of news pairs**: The script `create_pairs.py` generates pairs of articles (news) from a dataset covering one or more months. It uses similarity measures such as cosine similarity and Jaccard similarity to identify pairs of articles with similar subjects.

2. **Model training**: The `train_model.py` script trains a model from the SentenceTransformer library on a set of news pairs generated by the `create_pairs.py` script. This script refines a pre-trained model on news article pairs and saves the trained model for later use.

3. **Model evaluation**: The `model_evaluation.py` script assesses the trained model on a set of news pairs, predicting news subjects from news embeddings. It encodes articles using the refined model and trains a multi-output classifier to predict news subjects. Performance is evaluated by calculating standard measures: accuracy, precision, recall, and F1-score. Then, the performance of the refined model is compared to that of reference models (a random prediction model based on subject frequencies and a TF-IDF encoding approach).

4. **Model testing**: The `test_all_models.py` script is designed to test different models available in the SentenceTransformer library. It trains and evaluates several models on a dataset of news pairs. The models' performances are assessed during training. The script records the evaluation scores and training time of each model in a CSV file for later analysis.

5. **Data exploration**: The `data_exploration.ipynb` notebook allows for exploring the data used in the project. It offers the possibility to visualize the data, obtain statistics about the data, observe the distributions of subjects, etc.

6. **Model exploration**: The `model_exploration.ipynb` notebook facilitates the exploration of SentenceTransformers' pre-trained models by visualizing the training and evaluation performances of each model.

7. **Examples of using the model**: The `use_cases.ipynb` notebook presents examples of using the trained model, including visualization of embeddings, detection of trending subjects, and semantic search of news.


## User/Developer Manual (Installation and Execution)
> The project was developed and tested with Python 3.8.10 (Linux server) and 3.9.6 (on macOS)

At the beginning of each notebook and script, a **configuration** section defines various paths and script parameters through constant variables. These parameters can be modified to adapt the project to your needs, including paths to data and models, and model training settings.

There are 2 main scripts:  
- The `process.py` script generates data pairs, trains the model, and evaluates it. This file combines the 3 scripts `create_pairs.py`, `train_model.py`, and `model_evaluation.py`, thus simplifying the execution of the entire workflow.

- The `test_all_models.py` script tests different SentenceTransformer library models on a set of news pairs.

The notebooks allow for the exploration of data, evaluations of SentenceTransformer models, and viewing examples of using the trained model. For these notebooks, please ensure you have executed the entire workflow beforehand to have the necessary data for their proper functioning.

**Note: All scripts must be executed from the project root, so that the relative paths work correctly.**


### Project Structure (with all files and folders)
This section provides a description of the project's structure, as well as its folders and files.

> **Note:** The data in the `news-clustering/data/` and `news-clustering/model/` folders are not included in the repository due to their large volume. They need to be generated by running the corresponding scripts and importing the necessary data. However, the final fine-tuned model is available in the submitted version on the HE-Arc server in the `news-clustering/model/` folder. Also, the score files are not initially present in the repository, but they are automatically generated when executing the scripts.
>
> The presented structure is therefore that of the project after executing the scripts.

From the root `news-clustering/`, the folders and files are as follows:

- `model/`: This folder contains the trained model, relevant information about it, training and evaluation of the model, as well as the obtained scores and used hyperparameters.
  - `scores_evaluation.png`: Image with the model's evaluation scores during training.

- `data/`: This folder contains the data used for analysis, models, and results.
  - `news_train/`: Folder with training news data for the model.
  - `news_test/`: Folder with test news data for the model, used for evaluation and visualization of embeddings.
  - `pairs/`: Folder containing news pairs generated by the `create_pairs.py` script.
  - `news_code.csv`: File with correspondences between codes and news subjects.
  
- `notebook/`: This folder contains the notebooks used for project development.
  - `data_exploration.ipynb`: Notebook for data exploration.
  - `model_explore.ipynb`: Notebook for exploring SentenceTransformer library models, analyzing the performance of each model on a dataset of news pairs.
  - `use_cases.ipynb`: Notebook presenting examples of using the trained model.
  
- `util/`: This folder contains utilities used for the project.
  - `NewsProcessor.py`: Class for preprocessing news and generating news pairs.

- `script/`: This folder contains the main scripts used for the project.
  - `create_pairs.py`: Script for creating news pairs from a dataset.
  - `train_model.py`: Script for training a model from news pairs.
  - `model_evaluation.py`: Script for evaluating the trained model by classifying news.
  - `test_all_models.py`: Script for testing different models available in the SentenceTransformer library.
  
- `scores/`: This folder contains the results of evaluations of different models.
  - `classification_scores.csv`: File with the evaluation results of a fine-tuned model on a news dataset by comparing predictions with the actual news subjects, as well as the results of base models (model without fine-tuning, TF-IDF, and Random).
  - `models_scores.csv`: File with the evaluation results of different SentenceTransformers models trained on a dataset of news pairs.

- `doc/`: This folder contains the project documentation.
  - `create_pairs.html`: Documentation of the `create_pairs.py` script.
  - `train_model.html`: Documentation of the `train_model.py` script.
  - `model_evaluation.html`: Documentation of the `model_evaluation.py` script.
  - `test_all_models.html`: Documentation of the `test_all_models.py` script.
  - `process.html`: Documentation of the `process.py` script.
  - `NewsProcessor.html`: Documentation of the `NewsProcessor.py` class.

- `process.py`: Script that allows executing the entire main workflow (creation of pairs, model training, model evaluation).
- `requirements.txt`: File containing the project dependencies.
- `README.md`: File containing the project description, procedure for executing scripts and notebooks, and the project structure.



## Instructions for Using the Project
Before you can run certain files, some preliminary steps are necessary. Here is the procedure:

**1** - First, clone the project from the repository and navigate to the project root:
```sh
git clone git@gitlab-etu.ing.he-arc.ch:isc/2022-23/niveau-3/3285-tb-id/203/news-clustering.git

cd news-clustering
```
  
**2** - Next, install the project dependencies, preferably in a virtual environment. To do this, execute the following commands from the project root:
```sh
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

**3** - Place the training data in the `news-clustering/data/news_train` folder and the test data in the `news-clustering/data/news_test` folder (these are the default paths and can be modified). The data must be in JSON format. For each folder, you can place one or more JSON files, each representing a month, according to your processing preferences.

**4** - After installing the dependencies, run the process.py script to generate data pairs, train the model, and evaluate it:
```sh
python process.py
```
- Alternatively, it is possible to run the scripts separately, in this order, to generate data pairs, train the model, and evaluate it:
```sh
python script/create_pairs.py
python script/train_model.py
python script/model_evaluation.py
```
- By default, the generated news pairs are saved in the `news-clustering/data/pairs/` folder, and the trained model is saved in the `news-clustering/model/` folder. The model evaluation results are saved in the `news-clustering/scores/classification_scores.csv` file.

**5** - To test different models from the SentenceTransformer library on a set of news pairs, run the `test_all_models.py` script:
```sh
python script/test_all_models.py
```
- By default, the results of the different models are saved in the `news-clustering/scores/models_scores.csv` file.

**6** - Finally, to explore the data, the models, and view examples of using the trained model, you can run the `data_exploration.ipynb`, `model_exploration.ipynb`, and `use_cases.ipynb` notebooks. Before doing this, make sure you have followed the entire workflow described in the guide, as this will ensure you have the necessary data and models to successfully execute these notebooks.

**7** - Additionally, all the documentation for this project is available in the `news-clustering/docs/` folder in HTML format.



## References
- [SentenceTransformer](https://www.sbert.net/)
- [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)
- [Project Wiki](https://gitlab-etu.ing.he-arc.ch/groups/isc/2022-23/niveau-3/3285-tb-id/203/-/wikis/home)
